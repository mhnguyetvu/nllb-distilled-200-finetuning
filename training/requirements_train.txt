# Training dependencies for A100 server
transformers>=4.36.0
datasets>=2.16.0
torch>=2.1.0
accelerate>=0.25.0
evaluate>=0.4.1
sacrebleu>=2.3.1
sentencepiece>=0.1.99
protobuf>=3.20.0
tensorboard>=2.15.0
scipy>=1.11.0
scikit-learn>=1.3.0

# Optional but recommended for A100
flash-attn>=2.3.0  # Flash Attention 2 for 2-3x speedup
bitsandbytes>=0.41.0  # Memory optimization
deepspeed>=0.12.0  # Distributed training (if multi-GPU)

# Evaluation metrics
unbabel-comet>=2.2.0  # COMET metric for better evaluation
bert-score>=0.3.13  # BERTScore metric
