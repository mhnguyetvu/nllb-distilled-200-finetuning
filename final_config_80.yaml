model:
  name: facebook/nllb-200-distilled-600M
  src_lang: kor_Hang
  tgt_lang: vie_Latn

data:
  train_file: /data/AITeam/nguyetnvm/nllb/data/sweep/semantic_80/nllb_train.jsonl
  dev_file: /data/AITeam/nguyetnvm/nllb/data/sweep/semantic_80/nllb_dev.jsonl
  test_file: /data/AITeam/nguyetnvm/nllb/data/sweep/semantic_80/nllb_test.jsonl
  max_length: 256
  max_source_length: 256
  max_target_length: 256

training:
  output_dir: /data/AITeam/nguyetnvm/nllb/outputs/final_semantic_80
  num_train_epochs: 10
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 5.0e-05
  warmup_steps: 500
  logging_steps: 100
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3
  gradient_accumulation_steps: 1
  bf16: true
  dataloader_num_workers: 4
  remove_unused_columns: false
  load_best_model_at_end: true
  metric_for_best_model: bleu
  greater_is_better: true
  log_dir: /data/AITeam/nguyetnvm/nllb/logs/final

generation:
  num_beams: 5
  max_length: 256
  early_stopping: true

optimization:
  gradient_checkpointing: true
  optim: adamw_torch_fused
